$ cat /posts/cve-2021-4204-linux-kernel-ebpf-lpe.txt

|=------=[ CVE-2021-4204 / tr3e ]=------=|

--[ 1 - TL;DR

During a recent code audit of Linux eBPF, a privilege escalation vulnerability CVE-2021-4204 was discovered.

This vulnerability affects Linux Kernel 5.8 - 5.16.

For the complete exploit code, see: https://github.com/tr3ee/CVE-2021-4204

----[ 1.1 - eBPF verifier

To ensure the security of the kernel, all eBPF programs loaded into the kernel will be checked for legitimacy by the eBPF verifier.

Naturally, these checks include the legitimacy check of auxiliary functions. After all, one of the major features that distinguishes eBPF from BPF is the auxiliary function, which provides eBPF with a rich kernel function calling capability.

eBPF verifier requires auxiliary functions to provide the calling convention of the `struct bpf_func_proto` structure and performs a legitimacy check on the function caller according to the convention.

* C *
------------------------------------------------------------------------------------------------
const struct bpf_func_proto bpf_map_lookup_elem_proto = {
.func = bpf_map_lookup_elem,
.gpl_only = false,
.pkt_access = true,
.ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,
.arg1_type = ARG_CONST_MAP_PTR,
.arg2_type = ARG_PTR_TO_MAP_KEY,
};
------------------------------------------------------------------------------------------------

For example, in the above example, all programs calling bpf_map_lookup_elem are required to follow:

- Parameter 1: a map pointer

- Parameter 2: a pointer to a key

- Return value: may be a pointer to a value, or it may be NULL

So when you try to call `bpf_map_lookup_elem(NULL, NULL)`, the eBPF verifer will fail and tell you that the expected parameter is a map_ptr instead of a scalar.

Of course, this example is the simplest helper function. After all, it requires us to provide a pointer to the key, but not the length of the key, because
this length is already given when we define the map.

* C *
------------------------------------------------------------------------------------------------
const struct bpf_func_proto bpf_strtol_proto = {
.func = bpf_strtol,
.gpl_only = false,
.ret_type = RET_INTEGER,
.arg1_type = ARG_PTR_TO_MEM,
.arg2_type = ARG_CONST_SIZE,
.arg3_type = ARG_ANYTHING,
.arg4_type = ARG_PTR_TO_LONG,
};
------------------------------------------------------------------------------------------------

In this example, bpf_strtol is used to convert a string to a long type. It requires the program that calls bpf_strtol to comply with the following:

- Parameter 1: a pointer to a string

- Parameter 2: the size of the string

- Parameter 3: any non-pointer value

- Parameter 4: a pointer to a long type

- Return value: Integer

This is a bit more complicated than the previous one, because its input is a string of indefinite length, so the caller must provide the length of the string.

Based on this convention, eBPF verifier will verify this input during the checking phase to prevent the caller from providing invalid parameters to cause out-of-bounds memory reading and writing.

For example, eBPF verifier considers a call like `bpf_strtol("hello", 5, 0, &res)` to be legal. But if our call is `b
pf_strtol("hello", 100, 0, &res)`, it is obvious that the length of "hello" is not 100, so the program will be rejected.

Usually, when the parameters of auxiliary functions contain dynamically sized memory blocks, the calling convention will be followed by an integer to indicate the size of the memory block, just like
bpf_strtol_proto above.

*C* -------------------------------------------------------------------------------------------------------- 
static int check_func_arg(struct bpf_verifier_env *env, u32 arg,
                          struct bpf_call_arg_meta *meta,
                          const struct bpf_func_proto *fn)
{
...
    } else if (arg_type_is_mem_ptr(arg_type)) {
        /* The access to this pointer is only checked when we hit the
         * next is_mem_size argument below.
         */
        meta->raw_mode = (arg_type == ARG_PTR_TO_UNINIT_MEM);
    } else if (arg_type_is_mem_size(arg_type)) {
...
        err = check_helper_mem_access(env, regno - 1,
                                      reg->umax_value,
                                      zero_size_allowed, meta);
...
    }
...
    return err;
}
------------------------------------------------------------------------------------------------

In the above code, check_func_arg() is the specific implementation of eBPF verifier to check the calling convention.

You can see that if parameter X is mem_ptr and parameter X+1 is mem_size, check_helper_mem_access will be called to check the memory of parameter X.

However, is there a calling convention that does not follow this rule?

----[ 1.2 - Vulnerability Analysis

Yes, it exists! BPF ringbuf does not follow this rule, which is CVE-2021-4204.

BPF ringbuf is a built-in high-performance ring buffer used to make up for the scenarios that perf buffer cannot meet. For related documents, please see this
https://www.kernel.org/doc/html/latest/bpf/ringbuf.html

* C *
------------------------------------------------------------------------------------------------
BPF_CALL_2(bpf_ringbuf_submit, void *, sample, u64, flags)
{
bpf_ringbuf_commit(sample, flags, false /* discard */);
return 0;
}

const struct bpf_func_proto bpf_ringbuf_submit_proto = {
.func = bpf_ringbuf_submit,
.ret_type = RET_VOID,
.arg1_type = ARG_PTR_TO_ALLOC_MEM,
.arg2_type = ARG_ANYTHING,
};

BPF_CALL_2(bpf_ringbuf_discard, void *, sample, u64, flags)
{
bpf_ringbuf_commit(sample, flags, true /* discard */);
return 0;
}

const struct bpf_func_proto bpf_ringbuf_discard_proto = {
.func = bpf_ringbuf_discard,
.ret_type = RET_VOID,
.arg1_type = ARG_PTR_TO_ALLOC_MEM,
.arg2_type = ARG_ANYTHING,
};
------------------------------------------------------------------------------------------------

In the above ringbuf.c code, two auxiliary functions bpf_ringbuf_submit() and bpf_ringbuf_discard() are provided. Their
first parameter is mem_ptr and there is no mem_size type parameter.

The original intention of the BPF ringbuf design is to implement a high-throughput ring buffer. Under normal circumstances, its calling process in the kernel is:

[1] Call bpf_ringbuf_reserve() to obtain a buffer memory P

[2] Write to the buffer memory P

[3] Call bpf_ringbuf_submit() to submit data or call bpf_ringbuf_discard() to discard data

This process is called data production, and usually there is a program in user mode that continuously reads the data it produces. This process is called data consumption.

bpf_ringbuf_{submit, discard} is intended to submit/discard the data we produce, but it does not check the pointer parameter passed in.
This allows us to pass in an out-of-bounds pointer when calling bpf_ringbuf_{submit, discard}, and use its internal logic to achieve pollution, thereby executing code in the kernel and elevating permissions.

----[ 1.3 - Vulnerability Exploitation

The function bpf_ringbuf_{submit, discard} finally calls bpf_ringbuf_commit(), which is implemented as follows:

* C *
------------------------------------------------------------------------------------------------
static void bpf_ringbuf_commit(void *sample, u64 flags, bool discard)
{
    unsigned long rec_pos, cons_pos;
    struct bpf_ringbuf_hdr *hdr;
    struct bpf_ringbuf *rb;
    u32 new_len;

    hdr = sample - BPF_RINGBUF_HDR_SZ;
    rb = bpf_ringbuf_restore_from_rec(hdr);
    new_len = hdr->len ^ BPF_RINGBUF_BUSY_BIT;
    if (discard)
        new_len |= BPF_RINGBUF_DISCARD_BIT;

    /* update record header with correct final size prefix */
    xchg(&hdr->len, new_len);

    /* if consumer caught up and is waiting for our record, notify about
     * new data availability
     */
    rec_pos = (void *)hdr - (void *)rb->data;
    cons_pos = smp_load_acquire(&rb->consumer_pos) & rb->mask;

    if (flags & BPF_RB_FORCE_WAKEUP)
        irq_work_queue(&rb->work);
    else if (cons_pos == rec_pos && !(flags & BPF_RB_NO_WAKEUP))
        irq_work_queue(&rb->work);
}
------------------------------------------------------------------------------------------------

bpf_ringbuf_commit() will do the following:

[1] *(sample - BPF_RINGBUF_HDR_SZ) ^= BPF_RINGBUF_BUSY_BIT

[2] If discard is true, *(sample - BPF_RINGBUF_HDR_SZ) |= BPF_RINGBUF_DISCARD_BIT

[3] Call irq_work_queue(&rb->work) when the conditions are met

So how is it implemented inside the ringbuf? Here is an explanation. The internal implementation of bpf_ringbuf can be seen in the following structure definition:


* C *
------------------------------------------------------------------------------------------------
struct bpf_ringbuf {
    wait_queue_head_t waitq;
    struct irq_work work;
    u64 mask;
    struct page **pages;
    int nr_pages;
    spinlock_t spinlock ____cacheline_aligned_in_smp;
    /* Consumer and producer counters are put into separate pages to allow
     * mapping consumer page as r/w, but restrict producer page to r/o.
     * This protects producer position from being modified by user-space
     * application and ruining in-kernel position tracking.
     */
    unsigned long consumer_pos __aligned(PAGE_SIZE);
    unsigned long producer_pos __aligned(PAGE_SIZE);
    char data[] __aligned(PAGE_SIZE);
};
------------------------------------------------------------------------------------------------

The data field in bpf_ringbuf is our ring buffer, and the mask field is the mask used to mark the ring buffer.

In addition, consumer_pos is used to track the consumer's consumption progress, and producer_pos is used to track the producer's production progress.

However, since bpf_ringbuf_{submit, discard} does not check the input pointer, we can point the pointer that should have pointed to data to the mask field through arithmetic operations, and by changing the mask field, we have the ability to read and write out of bounds in disguise.

However, since we can only get the memory pointer in data through bpf_ringbuf_reserve(), we cannot directly control the fields of bpf_ringbuf forward. However, fortunately, we can create a large number of bpf_ringbufs and use the heap spray method to read and write the next bpf_ringbuf backward out of bounds, which allows us to fully control the bpf_ringbuf structure behind.

After we have complete control over bpf_ringbuf, let's take a look at how bpf_ringbuf_reserve works?

* C *
------------------------------------------------------------------------------------------------
static void *__bpf_ringbuf_reserve(struct bpf_ringbuf *rb, u64 size)
{
    unsigned long cons_pos, prod_pos, new_prod_pos, flags;
    u32 len, pg_off;
    struct bpf_ringbuf_hdr *hdr;

    if (unlikely(size > RINGBUF_MAX_RECORD_SZ))
        return NULL;

    len = round_up(size + BPF_RINGBUF_HDR_SZ, 8);
    if (len > rb->mask + 1)
        return NULL;

    cons_pos = smp_load_acquire(&rb->consumer_pos);

    if (in_nmi()) {
        if (!spin_trylock_irqsave(&rb->spinlock, flags))
            return NULL;
    } else {
        spin_lock_irqsave(&rb->spinlock, flags);
    }

    prod_pos = rb->producer_pos;
    new_prod_pos = prod_pos + len;

    /* check for out of ringbuf space by ensuring producer position
     * doesn't advance more than (ringbuf_size - 1) ahead
     */
    if (new_prod_pos - cons_pos > rb->mask) {
        spin_unlock_irqrestore(&rb->spinlock, flags);
        return NULL;
    }

    hdr = (void *)rb->data + (prod_pos & rb->mask);
    pg_off = bpf_ringbuf_rec_pg_off(rb, hdr);
    hdr->len = size | BPF_RINGBUF_BUSY_BIT;
    hdr->pg_off = pg_off;

    /* pairs with consumer's smp_load_acquire() */
    smp_store_release(&rb->producer_pos, new_prod_pos);

    spin_unlock_irqrestore(&rb->spinlock, flags);

    return (void *)hdr + BPF_RINGBUF_HDR_SZ;
}

BPF_CALL_3(bpf_ringbuf_reserve, struct bpf_map *, map, u64, size, u64, flags)
{
    struct bpf_ringbuf_map *rb_map;

    if (unlikely(flags))
        return 0;

    rb_map = container_of(map, struct bpf_ringbuf_map, map);
    return (unsigned long)__bpf_ringbuf_reserve(rb_map->rb, size);
}
------------------------------------------------------------------------------------------------

As you can see, the main operation to get the memory block here is `rb->data + (prod_pos & rb->mask)`, and both prod_pos and mask are controlled by us. When mask=0xffffffffffffffff, this operation is equivalent to allocating any address through prod_pos, thereby obtaining any address read and write.

Unfortunately, after completing the allocation, it also performs two write operations to the allocated address: `hdr->len = size | BPF_RINGBUF_BUSY_BIT`,
`hdr->pg_off = pg_off`, which greatly reduces our ability to allocate any address and has data pollution problems. This can only be regarded as limited address read and write.

At the same time, although we have obtained kernel address read and write, there is no related address leakage, and the necessary kernel address is still needed to complete the next step.

Therefore, we turn to the bpf_ringbuf structure to see if we can get some useful address information.


* GDB *
------------------------------------------------------------------------------------------------
gef> x/32gx $rdi
0xffffc90000517000:     0x0000000000000000      0xffffc90000517008
0xffffc90000517010:     0xffffc90000517008      0x0000000000000000
0xffffc90000517020:     0x0000000000000000      0xffffffff811361e0
0xffffc90000517030:     0x0000000080000fff      0xffff888003cd8680
0xffffc90000517040:     0x0000000000000004      0x0000000000000000
0xffffc90000517050:     0x0000000000000000      0x0000000000000000
0xffffc90000517060:     0x0000000000000000      0x0000000000000000
0xffffc90000517070:     0x0000000000000000      0x0000000000000000
0xffffc90000517080:     0x0000000000000000      0x0000000000000000
0xffffc90000517090:     0x0000000000000000      0x0000000000000000
0xffffc900005170a0:     0x0000000000000000      0x0000000000000000
0xffffc900005170b0:     0x0000000000000000      0x0000000000000000
0xffffc900005170c0:     0x0000000000000000      0x0000000000000000
0xffffc900005170d0:     0x0000000000000000      0x0000000000000000
0xffffc900005170e0:     0x0000000000000000      0x0000000000000000
0xffffc900005170f0:     0x0000000000000000      0x0000000000000000
gef> p *(struct bpf_ringbuf*) $rdi
$4 = {
  waitq = {
    lock = {
      {
        rlock = {
          raw_lock = {
            {
              val = {
                counter = 0x0
              },
              {
                locked = 0x0,
                pending = 0x0
              },
              {
                locked_pending = 0x0,
                tail = 0x0
              }
            }
          }
        }
      }
    },
    head = {
      next = 0xffffc90000517008,
      prev = 0xffffc90000517008
    }
  },
  work = {
    {
      node = {
        llist = {
          next = 0x0 <fixed_percpu_data>
        },
        {
          u_flags = 0x0,
          a_flags = {
            counter = 0x0
          }
        },
        src = 0x0,
        dst = 0x0
      },
      {
        llnode = {
          next = 0x0 <fixed_percpu_data>
        },
        flags = {
          counter = 0x0
        }
      }
    },
    func = 0xffffffff811361e0 <bpf_ringbuf_notify>
  },
  mask = 0x80000fff,
  pages = 0xffff888003cd8680,
  nr_pages = 0x4,
  spinlock = {
    {
      rlock = {
        raw_lock = {
          {
            val = {
              counter = 0x0
            },
            {
              locked = 0x0,
              pending = 0x0
            },
            {
              locked_pending = 0x0,
              tail = 0x0
            }
          }
        }
      }
    }
  },
  consumer_pos = 0x0,
  producer_pos = 0xff8,
  data = 0xffffc9000051a000 "\360\017"
}
------------------------------------------------------------------------------------------------

The above is a typical structure of bpf_ringbuf in memory. Because `bpf_ringbuf->data` maps a group of fragmented memory to continuous virtual memory through vmap, data is located in `vmalloc space`. Both task_struct and cred of the process are allocated in `direct mapping space`, which means that even if we get the address of bpf_ringbuf, we cannot escalate privileges because we do not have the address of `direct mapping space`.

However, there is a special field in bpf_ringbuf - `bpf_ringbuf->pages`, which retains a group of fragmented memory arrays, that is, the real address of data, and the address of this array is located in `direct mapping space`, so we have the leak we need, but the data pollution problem in arbitrary address allocation is still not solved. But didn't we just say that `bpf_ringbuf->pages` is an array? Let's take a look at its layout in memory.


* GDB *
------------------------------------------------------------------------------------------------
gef> x/6gx ((struct bpf_ringbuf*)$rdi)->pages
0xffff888003cd8680:     0xffffea00000f3ac0      0xffffea00000f3b00
0xffff888003cd8690:     0xffffea00000f3b40      0xffffea00000f3b80
0xffff888003cd86a0:     0xffffea00000f3b80      0x0000000000000000
0xffff888003cd86b0:     0x0000000000000000      0x0000000000000000
------------------------------------------------------------------------------------------------


Sure enough, the sixth element of the array is 0x00000000000000000, so we can use the above arbitrary address allocation to allocate the ring buffer at the location of `pages+0x28`, so we can read and write the memory after `pages+0x30`. Then by heap spraying task_struct, we can successfully rewrite the cred of the process.

The exploit is written casually, and I will add more later when I have the chance.

For the complete exploit code, please see: https://github.com/tr3ee/CVE-2021-4204

----[ 1.4 - References

[1] Advisory: https://www.openwall.com/lists/oss-security/2022/01/11/4

[2] Exploit Overview: https://www.openwall.com/lists/oss-security/2022/01/18/1

[3] Patch: [bpf: Fix out of bounds access for ringbuf helpers](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=64620e0a1e712a778095bd35cbb277dc2259281f)

[4] Source: https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/ver ifier.c?h=v5.10.83 [5] CVE-2020-8835: LINUX KERNEL PRIVILEGE ESCALATION VIA IMPROPER EBPF PROGRAM VERIFICATION: htt ps://www.zerodayinitiative.com/blog/2020/4/8/cve-2020-8835-linux-kernel-privilege-escalation-via -improper-ebpf-program-verification